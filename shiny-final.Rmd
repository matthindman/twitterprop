---
title: "shiny-final"
author: "Yuzhou Tu"
date: "06/18/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,global}
install.packages("shiny")
install.packages("DBI")
install.packages("RMySQL")
devtools::install_github("rstudio/pool")
install.packages('rsconnect')
install.packages("shinythemes")
install.packages("DT")
install.packages("tokenizer")
install.packages("reshape2")
install.packages("tidyr")
install.packages("dplyr")
install.packages("magrittr")
install.packages("tm")
install.packages("wordcloud")
install.packages("igraph")
install.packages("ggraph")
library(rsconnect)
library(shiny)#Shiny App
library(DBI)#Create a connection to a DBMS
library("RMySQL")#Create a connection to a SQLite Database
library(pool)#Enables the creation of object pools, which make it less computationally expensive to fetch a new object. Currently the only supported pooled objects are 'DBI' connections.
library(tm)#Topic Modeling
library(wordcloud)#Word Cloud
library(dplyr)
library(stringr)
library(tokenizers)#Convert natural language text into tokens.
library(tidytext)#Convert text to tidy formats
library(magrittr)#To use pipe operators like %>%
library(tidyr)#Seperate bigrams to clean stopwords
library(reshape2)#Use acast
library(igraph)
library(ggraph)
library(DT)#Create table in Shiny
library(shinythemes)#Theme selection
get_sentiments("bing")
#please replace the directory in the following command with your own directory of fake.db
pool <- dbPool(
  drv = RSQLite :: SQLite(),
  dbname = "/Users/Duckburg/Dropbox/2018stat/6289-consulting/project2/shiny-fakeaccounts/Data/fake.db"
)
#Define 6 groups
group <<- list("Trump Support" = "Trump Support",
               "Conservative" = "Conservative",
               "Hard Conservative"="Hard Conservative",
               "Intl Right|Anti-Islam"="Intl Right|Anti-Islam",
               "Other"="Other",
               "Russia" = "Russia")
data(stop_words)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
SEED = 2018;
#create function to clean tweet corpus 
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(tolower))#transfer to lower case
  corpus <- tm_map(corpus, removeWords, stopwords("en"))#remove stopwords
  corpus <- tm_map(corpus, stripWhitespace)#remove extra whitespace 
  corpus <- tm_map(corpus, stemDocument)#stem words
  return(corpus)
}
```


```{r,ui}
ui <- fluidPage(theme = shinytheme("cerulean"),
  tabsetPanel(
    tabPanel("Text Mining",fluid=TRUE,
  sidebarLayout(
    sidebarPanel(
      selectInput("group", "Choose a group:",
                  choices = group),
      helpText("Choose the date range"),  
      dateInput("date1", "From Date:",value='2016-10-01'),
      dateInput("date2", "To Date:",value='2016-11-01'),
      hr(),
      sliderInput("freq",
                  "Minimum Frequency:",
                  min = 1,  max = 2000, value = 50),
      sliderInput("max",
                  "Maximum Number of Words:",
                  min = 1,  max = 300,  value = 100)
    ),
    mainPanel(
      tabsetPanel(type = "tabs",
                  tabPanel("Summary Statistics",dataTableOutput("Summary")),
                  tabPanel("Word Cloud", plotOutput("plot")),
                  tabPanel("Sentiment Analysis", plotOutput("plot3"),plotOutput("plot2")),
                  tabPanel("Bigram", plotOutput("plot4")))
  ))),
  tabPanel("Topic Modeling",fluid=TRUE,
  sidebarLayout(
  sidebarPanel(
      #actionButton("update", "Change"),
      dateInput("date3", "From Date:",value='2016-10-01'),
      dateInput("date4", "To Date:",value='2016-11-01'),
      hr(),
      numericInput("topic", label = h3("Number of topics"), value =2,max=10),
      numericInput("words", label = h3("Top n words in each topic"), value =10,max = 30),
      downloadButton('downloadData', 'Download')
    ),
    # Show Top Word
    mainPanel(
      tabsetPanel(type = "tabs",
                  tabPanel("Top Words in Each Topic",dataTableOutput('table'))

      )
    )
)
)))
```

```{r,server}
server <- function(input, output, session) {
  ##Part 1:summary table to summarize number of tweets sent by users from each group
  output$Summary = renderDataTable({
    sql <- "SELECT text,user_id,group_name FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2;"
    query <- sqlInterpolate(pool,sql,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
    df <- dbGetQuery(pool, query)
    #produce table
    datatable(as.data.frame(table(df$group_name)),colnames = c("Group", "Number of Tweets"))
})
  ##Part 2:word cloud
  output$plot <- renderPlot({
    sql <- "SELECT text FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
    query <- sqlInterpolate(pool, sql, group = input$group,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
    df <- dbGetQuery(pool, query)
    text <- df$text
    text <- as.character(text)
    text <- iconv(text, 'utf-8', 'ascii', sub='') #convert character vector between encodings
    #remove all the retweet username and urls contained in tweets
    clean_tweet = gsub("&amp", "", text)
    clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
    clean_tweet = gsub("@\\w+", "", clean_tweet)
    clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
    clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
    clean_tweet = gsub("http\\w+", "", clean_tweet)
    clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
    clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
    #create corpus using cleaned tweets
    tweet_source <- VectorSource(clean_tweet)
    tweet_corpus <- VCorpus(tweet_source)
    tweet_corpus <- clean_corpus(tweet_corpus)
    #create term-document matrix
    myTDM = TermDocumentMatrix(tweet_corpus,control = list(minWordLength = 1))
    m = as.matrix(myTDM)
    #draw word cloud
    term_frequency<-rowSums(m)
    term_frequency<-sort(term_frequency,decreasing=TRUE)
    word_freqs<-data.frame(term=names(term_frequency),num=term_frequency)
    wordcloud(word_freqs$term, 
              word_freqs$num, 
              min.freq = input$freq,#input the setted min freq
              max.words = input$max,#input the setted max number of words
              colors = brewer.pal(8, "Dark2"))
  })
  ##Part 3:Sentiment Analysis-word cloud
    output$plot2 <- renderPlot({
      sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
      query  <- sqlInterpolate(pool, sql, group = input$group,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      tweet <- tbl_df(df)#create a dataframe table
      text2 <- as.character(tweet$text)
      text2 <- iconv(text2, 'utf-8', 'ascii', sub='')
      clean_tweet2 = gsub("&amp", "", text2)
      clean_tweet2 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet2)
      clean_tweet2 = gsub("@\\w+", "", clean_tweet2)
      clean_tweet2 = gsub("[[:punct:]]", "", clean_tweet2)
      clean_tweet2 = gsub("[[:digit:]]", "", clean_tweet2)
      clean_tweet2 = gsub("http\\w+", "", clean_tweet2)
      clean_tweet2 = gsub("[ \t]{2,}", "", clean_tweet2)
      clean_tweet2 = gsub("^\\s+|\\s+$", "", clean_tweet2)
      tweet$text <- clean_tweet2
      #convert dataframe tweet into tidytext
      tidy_tweet <- tweet %>%
        unnest_tokens(word, text)
      #remove word 'trump' from tidytext
      tidy_tweet <- tidy_tweet %>%
        anti_join(stop_words)%>%
        filter(word != "trump")
      #create sentiment word cloud
      tidy_tweet %>%
        inner_join(get_sentiments("bing")) %>%
        count(word, sentiment, sort = TRUE) %>%
        acast(word ~ sentiment, value.var = "n", fill = 0) %>%
        comparison.cloud(colors = c("gray20", "tomato"),
                         max.words = input$max)#set max number of words to be included
    })
    ##Part 4: Sentiment Analysis-histogram
    output$plot3 <- renderPlot({
      sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
      query  <- sqlInterpolate(pool, sql, group = input$group,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      tweet3 <- tbl_df(df)
      text3 <- as.character(tweet3$text)
      text3 <- iconv(text3, 'utf-8', 'ascii', sub='')
      clean_tweet3 = gsub("&amp", "", text3)
      clean_tweet3 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet3)
      clean_tweet3 = gsub("@\\w+", "", clean_tweet3)
      clean_tweet3 = gsub("[[:punct:]]", "", clean_tweet3)
      clean_tweet3 = gsub("[[:digit:]]", "", clean_tweet3)
      clean_tweet3 = gsub("http\\w+", "", clean_tweet3)
      clean_tweet3 = gsub("[ \t]{2,}", "", clean_tweet3)
      clean_tweet3 = gsub("^\\s+|\\s+$", "", clean_tweet3)
      tweet3$text <- clean_tweet3
      tidy_tweet <- tweet3 %>%
        unnest_tokens(word, text)
      tidy_tweet <- tidy_tweet %>%
        anti_join(stop_words)%>%
        filter(word != "trump")
      bing_word_counts <- tidy_tweet %>%
        inner_join(get_sentiments("bing")) %>%
        count(word, sentiment, sort = TRUE) %>%
        ungroup()
      #plot histogram
      bing_word_counts %>%
        group_by(sentiment) %>%
        top_n(input$max/2) %>%
        ungroup() %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(word, n, fill = sentiment)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~sentiment, scales = "free_y") +
        labs(y = "Contribution to sentiment",
             x = NULL) +
        coord_flip()
    })
    #Part 5:Bigram Analysis
    output$plot4 <- renderPlot({
      sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date1 AND ?date2 and group_name =?group;"
      query  <- sqlInterpolate(pool, sql, group = input$group,date1 = format(input$date1,format = "%Y-%m-%d"),date2=format(input$date2,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      tweet4 <- tbl_df(df)
      text4 <- as.character(tweet4$text)
      text4 <- iconv(text4, 'utf-8', 'ascii', sub='')
      clean_tweet4 = gsub("&amp", "", text4)
      clean_tweet4 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet4)
      clean_tweet4 = gsub("@\\w+", "", clean_tweet4)
      clean_tweet4 = gsub("[[:punct:]]", "", clean_tweet4)
      clean_tweet4 = gsub("[[:digit:]]", "", clean_tweet4)
      clean_tweet4 = gsub("http\\w+", "", clean_tweet4)
      clean_tweet4= gsub("[ \t]{2,}", "", clean_tweet4)
      clean_tweet4 = gsub("^\\s+|\\s+$", "", clean_tweet4)
      tweet4$text <- clean_tweet4
      #seperate tweets into bigrams
      tweet_bigrams <- tweet4 %>%
        unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
        na.omit()
      #clean stopwords in bigrmas
      bigrams_separated <- tweet_bigrams %>%
        separate(bigram, c("word1", "word2"), sep = " ")
      bigrams_filtered <- bigrams_separated %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word)
      bigram_counts <- bigrams_filtered %>% 
        count(word1, word2, sort = TRUE)
      #plot bigram network graph with bigrams with minimum frequency > input$freq
      bigram_graph <- bigram_counts %>%
        filter(n > input$freq) %>%
        graph_from_data_frame()
      ggraph(bigram_graph, layout = "fr") +
        geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                       arrow = a, end_cap = circle(.07, 'inches')) +
        geom_node_point(color = "lightblue", size = 5) +
        geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
        theme_void()
    })
    ##Part 6:Topic Modeling
    datasetInput <- reactive({
      sql <- "SELECT text,user_id FROM temp WHERE parsed_created_at BETWEEN ?date3 AND ?date4;"
      query <- sqlInterpolate(pool, sql,date3 = format(input$date3,format = "%Y-%m-%d"),date4=format(input$date4,format="%Y-%m-%d"))
      df <- dbGetQuery(pool, query)
      tweet5 <- tbl_df(df)
      text5 <- as.character(tweet5$text)
      text5 <- iconv(text5, 'utf-8', 'ascii', sub='')
      clean_tweet5 = gsub("&amp", "", text5)
      clean_tweet5 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet5)
      clean_tweet5 = gsub("@\\w+", "", clean_tweet5)
      clean_tweet5 = gsub("[[:punct:]]", "", clean_tweet5)
      clean_tweet5 = gsub("[[:digit:]]", "", clean_tweet5)
      clean_tweet5 = gsub("http\\w+", "", clean_tweet5)
      clean_tweet5 = gsub("[ \t]{2,}", "", clean_tweet5)
      clean_tweet5 = gsub("^\\s+|\\s+$", "", clean_tweet5)
      tweet5$text <- clean_tweet5
      tidy_tweet5 <- tweet5 %>%
        unnest_tokens(word, text)
      tidy_tweet5 <- tidy_tweet5 %>%
        anti_join(stop_words)
      tidy_tweet_id <- tidy_tweet5 %>%
        count(user_id, word)
      #create dtm according to user id
      new_dtm <- tidy_tweet_id %>%
        cast_dtm(user_id, word, n)
      rowTotals <- apply(new_dtm , 1, sum) #find the sum of words in each Document
      new_dtm <- new_dtm[rowTotals> 0, ]  #remove all docs without words
      #LDA with Gibbs Sampling
      tweet.lda <- LDA(new_dtm, input$topic, method="Gibbs", control=list(seed = SEED))
      #print the n most important words in the topic
      lda.terms <- as.table(terms(tweet.lda,input$words))
    }
    )
    #print the table
    output$table <- renderDataTable({
      datatable(datasetInput())
    })
    #download the table as csv
    output$downloadData <- downloadHandler(
      filename = function() {paste("Top",input$words,"words-", Sys.Date(), ".csv", sep="")},
      content = function(file) {
        write.csv(datasetInput(), file)
        })
}
```


```{r}
shinyApp(ui = ui, server = server)
```

